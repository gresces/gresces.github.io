<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gresces</title>
    <link>https://gresces.github.io/</link>
    <description>Recent content on Gresces</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>gresces.github.io</copyright>
    <lastBuildDate>Thu, 12 Jun 2025 19:25:11 +0800</lastBuildDate>
    <atom:link href="https://gresces.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scalene: Python分析器</title>
      <link>https://gresces.github.io/posts/scalene/</link>
      <pubDate>Thu, 12 Jun 2025 19:25:11 +0800</pubDate>
      <guid>https://gresces.github.io/posts/scalene/</guid>
      <description>&lt;p&gt;本文介绍一下SCALENE的实现原理。&lt;/p&gt;&#xA;&lt;p&gt;SCALENE是一个专门为Python设计的分析器，能够同时分析CPU时间、内存使用情况和GPU使用情况，并提供细粒度(行粒度)的性能信息。&lt;/p&gt;&#xA;&lt;p&gt;总之，按照文章的说明，它比之前的分析器都NB。&lt;/p&gt;&#xA;&lt;p&gt;文章：&lt;a href=&#34;https://www.usenix.org/conference/osdi23/presentation/berger&#34;&gt;OSDI 2023: Triangulating Python Performance Issues with Scalene&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;cpu-分析&#34;&gt;CPU 分析&lt;/h3&gt;&#xA;&lt;h4 id=&#34;传统方法&#34;&gt;传统方法&lt;/h4&gt;&#xA;&lt;p&gt;周期性地中断程序执行并检查当前的程序计数器（PC）。时间可以按照现实时间（CPU时间加IO等时间）计算，也可按照CPU时间（虚拟时间）计算。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;原理&lt;/strong&gt;：大样本情况下，PC所在进程的次数与进程执行时间成正比。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;局限1&lt;/strong&gt;：传统方法不能有效处理Python-本机代码交错执行的性能分析。在解释器没有获得控制权时，无法处理中断，所以执行本机代码时无法通过定时器中断进行抽样分析。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;局限2&lt;/strong&gt;：使用抽样方法难以分析子线程的执行时间。Python的信号由主线程处理，所以抽样仅对主线程计时，无法获得子线程的运行时间。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;  &lt;figure class=&#34;figure&#34;&gt;&#xA;    &lt;img class=&#34;img&#34; src=&#34;image.png&#34; alt=&#34;Figure1&#34; id=&#34;fig-Figure1&#34;&gt;&#xA;    &lt;figcaption class=&#34;center-align caption-text&#34;&gt;&#xA;      Figure1&#xA;      抽样分析器依赖于定时器中断，但由于Python在执行本机代码时会将中断信号延迟到解释器获得控制权时处理，所以执行本机代码的时无法进行抽样分析。在图中，T表示执行本机代码的时间，这一段时间将无法被计入执行时间内。&#xA;    &lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;h4 id=&#34;本文贡献&#34;&gt;本文贡献&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;技术1&lt;/strong&gt;：精确的Python-本机代码交错执行计时&lt;/p&gt;&#xA;&lt;p&gt;通过信号传递判断当前正在运行Python代码还是本机代码。如果中断信号被延迟处理，则正在运行本机代码，如&lt;a href=&#34;#fig-Figure1&#34;&gt;Figure1&lt;/a&gt;所示。在两次中断之间，通过虚拟时间time.process_time()之差得到本机代码的执行时间。在中断到来时，通过遍历Python堆栈来寻找当前正在执行的代码行，从而对运行时间归因，提供细粒度的分析结果。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;技术2&lt;/strong&gt;：精确的子线程计时&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;重新定义主线程的阻塞行为，通过使用猴子补丁的方式将threading.join等阻塞函数修改为循环等待函数。从而保证主线程能够一直处理定时器中断。&lt;/li&gt;&#xA;&lt;li&gt;为每个线程维护一个状态标志，主线程处理信号时，遍历所有线程，找到当前正在执行的线程，并获取其调用栈。&lt;/li&gt;&#xA;&lt;li&gt;通过字节码反汇编，通过CALL(CALL, CALL_FUNCTION, CALL_METHOD)来分析当前执行的代码行。如果执行Python代码，则很快就会进入跳转目标，否则若执行本机代码，则会一直停留在CALL指令上。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;内存分析&#34;&gt;内存分析&lt;/h3&gt;&#xA;&lt;h4 id=&#34;传统方法-1&#34;&gt;传统方法&lt;/h4&gt;&#xA;&lt;p&gt;Rate-Based Sampling：基于抽样，抽样的次数正比于分配或释放的内存数量，例如每分配或释放512KB采样一次。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;局限&lt;/strong&gt;：无法帮助定位内存泄漏。尽管可以记录内存分配的情况，以及被哪些代码占用了，可能存在内存问题，但不能说明这是内存泄漏。&lt;/p&gt;&#xA;&lt;h4 id=&#34;本文方法&#34;&gt;本文方法&lt;/h4&gt;&#xA;&lt;p&gt;&#xA;  &lt;figure class=&#34;figure&#34;&gt;&#xA;    &lt;img class=&#34;img&#34; src=&#34;image-1.png&#34; alt=&#34;Figure2&#34; id=&#34;fig-Figure2&#34;&gt;&#xA;    &lt;figcaption class=&#34;center-align caption-text&#34;&gt;&#xA;      Figure2&#xA;      两种采样方式的对比，菱形表示本文的采样方式，可以明显地看到采样次数的下降&#xA;    &lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;原理&lt;/strong&gt;：使用基于阈值的(Threshold-Based)采样而不是基于分配数量的采样。当内存的改变量大于设定阈值(|Alloca-Free|&amp;gt;Threshold)时，进行采样操作。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优势&lt;/strong&gt;：降低了采样次数，并减少了运行时开销。如&lt;a href=&#34;#fig-Figure2&#34;&gt;Figure2&lt;/a&gt;所示。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;拦截了系统内存分配器调用和Python的内置内存分配器，并替换为自己的shim分配器使用，&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/381694.378821&#34;&gt;已有代码&lt;/a&gt;改写系统分配器调用，并通过LD_PRELOAD(Linux)或者DYLD_INSERT_LIBRARIES(Mac OS X)注入。对于Python内部的分配器，使用Python的自定义API修改实现&lt;/li&gt;&#xA;&lt;li&gt;在基于阈值的采样触发时，追踪产生申请或释放内存的语句，并通过后台线程记录统计信息。&lt;/li&gt;&#xA;&lt;li&gt;在采样触发时，统计各个对象分配和释放的记录，并通过Laplace’s Rule of Succession来计算内存泄漏发生的可能性。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;gpu-分析&#34;&gt;GPU 分析&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt; 在配备NVIDIA GPU的系统上执行行级GPU利用率和内存分析，每进行一次CPU采样时，就收集当前使用的总GPU内存和利用率，并将其与当前执行的代码行相关联，从而帮助Python程序员确定他们是否有效地利用了GPU。&lt;/p&gt;&#xA;&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;&#xA;&lt;h4 id=&#34;亮点&#34;&gt;亮点&lt;/h4&gt;&#xA;&lt;p&gt;本文的亮点是在CPU和内存上的抽样工作。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;主要是CPU上的抽样，对于原生代码/外部库的运行时间区分以及多线程之间的运行时间区分，通过针对Python解释器的语言特性进行特殊化的处理，有可能可以拓展到其他的脚本/解释型语言上。&lt;/li&gt;&#xA;&lt;li&gt;而对于内存的抽样，则体现在新的抽样策略的改进，并对于之前的分析器难以进行的内存泄漏检测提出了解决方案。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;不足&#34;&gt;不足&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;对于GPU的分析像是狗尾续貂，强行增加来说明自己工作的NB。&lt;/li&gt;&#xA;&lt;li&gt;CPU的抽样只是基于某行代码的运行时间，未必能够表示CPU的实际占用率，例如sleep的执行。&lt;/li&gt;&#xA;&lt;li&gt;内存分析中的泄漏检测是基于概率，还需要进一步地核实是否发生了内存泄漏，即有误报，同时无法证明没有漏报。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/conference/osdi23/presentation/berger&#34;&gt;OSDI 2023: Triangulating Python Performance Issues with Scalene&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/642939534&#34;&gt;OSDI 2023论文评述 Day1&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>基于n8n的MCP实用构建</title>
      <link>https://gresces.github.io/posts/agent-in-n8n/</link>
      <pubDate>Tue, 10 Jun 2025 23:46:32 +0800</pubDate>
      <guid>https://gresces.github.io/posts/agent-in-n8n/</guid>
      <description>&lt;p&gt;本文提供：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在n8n中构建一个agent工作流&lt;/li&gt;&#xA;&lt;li&gt;在n8n中添加自定义mcp client&lt;/li&gt;&#xA;&lt;li&gt;使用python构建mcp server并在n8n工作流中连接&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;0-背景&#34;&gt;0. 背景&lt;/h2&gt;&#xA;&lt;p&gt;本文读者一般对MCP有一定了解，这里不进行解释。如果你需要知道背景知识，请看：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/29001189476&#34;&gt;LastWhisper：MCP (Model Context Protocol)，一篇就够了。&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文使用n8n构建一个完整的agent工作流，需要你在本机或服务器上部署一个n8n实例，当然如果你使用其他flow工具，可以参考本文章的mcp server构建部分。本文不提供n8n部署相关知识。&lt;/p&gt;&#xA;&lt;p&gt;注意，最好使用高于1.88版本的n8n，否则使用deepseek作为LLM时可能会有&lt;a href=&#34;https://github.com/n8n-io/n8n/issues/14574&#34;&gt;422报错&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1905634565013369483&#34;&gt;GeGarron：n8n 全方位安装指南：从本地部署到云端运行&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-在n8n中构建一个agent工作流&#34;&gt;1. 在n8n中构建一个agent工作流&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-创建一个工作流&#34;&gt;1.1 创建一个工作流&lt;/h3&gt;&#xA;&lt;p&gt;在n8n中通常是以工作流为单位进行工作的，在本例中，一个完整的聊天agent即为一个工作流。&#xA;在overview界面点击右上角的&lt;strong&gt;Create Workflow&lt;/strong&gt;创建一个工作流。&#xA;在n8n中，一个工作流的开始一般是一个触发器，点击中间的 + 图标，有很多触发器可供选择，这里选择&lt;strong&gt;On chat message&lt;/strong&gt;触发器即可，这个触发器在聊天框输入语句进行触发。&#xA;&#xA;  &lt;figure class=&#34;figure&#34;&gt;&#xA;    &lt;img class=&#34;img&#34; src=&#34;image.png&#34;&gt;&#xA;    &lt;figcaption class=&#34;center-align caption-text&#34;&gt;&#xA;      &#xA;      选择这个trigger&#xA;    &lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-创建agent&#34;&gt;1.2 创建agent&lt;/h3&gt;&#xA;&lt;p&gt;在创建完一个最基础的工作流之后，需要丰富这个工作流的内容，特别是加入LLM进行工具调用指导。&lt;/p&gt;&#xA;&lt;p&gt;点击第一个trigger节点的加号，创建第二个节点，输入agent，选择&lt;strong&gt;AI Agent&lt;/strong&gt;，之后点击左上角退回画布，可以发现多了一个Agent节点。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;  &lt;figure class=&#34;figure&#34;&gt;&#xA;    &lt;img class=&#34;img&#34; src=&#34;image-1.png&#34;&gt;&#xA;    &lt;figcaption class=&#34;center-align caption-text&#34;&gt;&#xA;      &#xA;      选择AI Agent&#xA;    &lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;可以发现，在Agent节点中，我们可以自定义的子节点（这玩意儿在n8n中叫sub-node）有：Chat Model、Memory和Tool。这几个子节点分别表示：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chat Model 接入的大语言模型，输入用户输入后输出工具调用信息，输入工具返回信息输出加工后的信息&lt;/li&gt;&#xA;&lt;li&gt;Memory 记录对话内容，提供上下文功能&lt;/li&gt;&#xA;&lt;li&gt;Tool 工具，agent&amp;quot;调用&amp;quot;的过程（mcp client、n8n节点或者n8n工作流）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;![](image-2.png &amp;ldquo;AI Agent节点的&amp;quot;接口&amp;rdquo;&amp;quot;)&lt;/p&gt;&#xA;&lt;p&gt;下面我们将逐项配置这三项：&lt;/p&gt;&#xA;&lt;h4 id=&#34;chat-model&#34;&gt;Chat Model&lt;/h4&gt;&#xA;&lt;p&gt;起手式点击 + 号，可以看到n8n提供了一系列的聊天模型节点供使用，比如DeepSeek和OpenAI。由于DeepSeek提供的API和OpenAI的API形式上相同，所以如果你选择DeepSeek模型，这两种节点都可以使用，本例使用OpenAI节点接入DeepSeek API。&lt;/p&gt;&#xA;&lt;p&gt;国内使用&amp;quot;SuckDick&amp;quot;模型非常方便，在&lt;a href=&#34;https://platform.deepseek.com/&#34;&gt;DeepSeek开放平台&lt;/a&gt;注册后获取一个API key即可。&lt;/p&gt;&#xA;&lt;p&gt;双击加入的OpenAI节点，在Credential下拉菜单中选择&lt;strong&gt;创建新的凭证&lt;/strong&gt;，填入获取到的API key，并按照&lt;a href=&#34;https://api-docs.deepseek.com/&#34;&gt;DeepSeek文档&lt;/a&gt;中提示的修改Base URL即可。&lt;/p&gt;&#xA;&lt;p&gt;&#xA;  &lt;figure class=&#34;figure&#34;&gt;&#xA;    &lt;img class=&#34;img&#34; src=&#34;image-3.png&#34;&gt;&#xA;    &lt;figcaption class=&#34;center-align caption-text&#34;&gt;&#xA;      &#xA;      点就完了&#xA;    &lt;/figcaption&gt;&#xA;  &lt;/figure&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;![](image-4.png &amp;ldquo;只需要修改API Key和Base URL，成功后会有绿色提示&amp;quot;Connection tested successfully&amp;rdquo;&amp;quot;)&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
